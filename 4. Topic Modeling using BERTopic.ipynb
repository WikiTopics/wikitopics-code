{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6146260,"sourceType":"datasetVersion","datasetId":3521629},{"sourceId":212593066,"sourceType":"kernelVersion"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Bertopic Modeling**\n\nBelow is the implementation of how I used Bertopic to topic model the wikinews articles","metadata":{}},{"cell_type":"code","source":"%pip install bertopic\nfrom bertopic import BERTopic\n\nimport pandas as pd\nimport numpy as np\nimport json\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Initially attempted to model by prefitting the BERTopic model to a sample of Wikipedia articles, so that it would better assign topics to the WikiNews articles. However, due to Kaggle memory limits, I was unable to sample more than a small random subset of Wikipedia, which unfortunately was worse than just directly fitting the model on the WikiNews articles.","metadata":{}},{"cell_type":"code","source":"import dask.dataframe as dd\nfrom dask.diagnostics import ProgressBar\n\ndir = '/kaggle/input/wikipedia-20230701'\n\nfile_paths = sorted(os.listdir(dir))\nfile_paths.remove('wiki_2023_index.parquet')\nfile_paths = [os.path.join(dir, path) for path in file_paths]\n\nfraction = 0.01\nbig_data = dd.read_parquet(file_paths[0]).sample(frac=fraction, random_state=42)\n\nfor file in file_paths[1:]:\n    curr = dd.read_parquet(file).sample(frac=fraction, random_state=42)\n    big_data = dd.concat([big_data, curr], ignore_index=True)\n    del curr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Attempted to topic model by first premodelling topics from subset of wikipedia\n# however was not able to sample a particularly large section of wikipedia to to memory constraints\n# ended up with more topic outliers\nbig_data_pd = big_data.compute()\nfit_docs = big_data_pd['text'].tolist()\nmodel = BERTopic()\n_ = model.fit_transform(fit_docs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The above model had around 11,000 articles marked with the -1 topic, indicating that they were unassigned. With the direct modelling approach, I was able to get around 6,900 articles marked -1, which is a much better improvement. Since we only had around 20,000 articles, minimizing this number is paramount.","metadata":{}},{"cell_type":"code","source":"# Direct generated topic modelling from just articles\n\nfile_path = '/kaggle/input/wikinews-data-converter-2-final-stage-3/enwikinews-processed.parquet'\n\nw_data = pd.read_parquet(file_path)\nw_data.drop(columns=['page_namespace'], inplace=True)\nw_data.dropna(inplace=True)\n\ndocs = w_data['page_text_extract_result'].tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model2 = BERTopic()\ntopics, probs = model2.fit_transform(docs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Topics represent the assigned topic number for a given document, while probs represents that documents probability of having that topic.","metadata":{}},{"cell_type":"code","source":"pd.value_counts(topics) # can see number of articles assigned topic -1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# add back to dataframe\n\nw_data['assigned_topic_num'] = topics\nw_data['topic_probability'] = probs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below I cleaned the date-related data in the dataframe for writing to a JSON for the website. I also outputted the topic models as a parquet for data analysis.\nThe output JSON/Dataframe contains each document as a row/entry, with the documents assigned topic, topic probability, date, and article title.","metadata":{}},{"cell_type":"code","source":"topical = w_data[w_data['assigned_topic_num'] != -1]\ntopical.drop(columns=['revision_id', 'page_id', 'page_text'], axis=1, inplace=True)\n\nimport re\n\ndef clean_dates(text):\n    pattern = r'\\b([jJ]anuary|[fF]ebruary|[mM]arch|[aA]pril|[mM]ay|[jJ]une|[jJ]uly|[aA]ugust|[sS]eptember|[oO]ctober|[nN]ovember|[dD]ecember) (\\d{1,2}), (\\d{4})\\b'\n    match = re.findall(pattern, text[0].lower())\n    if match:\n        return {\n            \"Year\": match[0][0],\n            \"Month\": match[0][1],\n            \"Day\": match[0][2],\n            \"Hour\": 0,\n            \"Minute\": 0,\n            \"Second\": 0\n            }\n    else:\n        return np.NaN\ntopical['article_date'] = topical['page_dates'].apply(clean_dates)\ntopical.dropna(inplace=True)\n\nfrom dateutil.parser import parse\n\ndef parsedatetodict(timestamp):\n    try:\n        dt = parse(timestamp)\n        return {\n            \"Year\": dt.year,\n            \"Month\": dt.month,\n            \"Day\": dt.day,\n            \"Hour\": dt.hour,\n            \"Minute\": dt.minute,\n            \"Second\": dt.second\n            }\n    except Exception as e:\n        return np.NaN\n\n\ntopical['page_dates_parsed'] = topical['page_dates_parsed'].apply(lambda x: parsedatetodict(x[0]))\ntopical['last_update_timestamp'] = topical['last_update_timestamp'].apply(lambda x: parsedatetodict(x[0]))\ntopical.dropna(inplace=True)\n\ntopical.drop(columns=['page_dates', 'page_text_extract_result'], inplace=True)\n\ntopical.reset_index(drop=True, inplace=True)\n\ntopical.to_json('topical_output.json', orient='records')\ntopical.to_parquet('topical_output.parquet', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here I produced the dict to assign the topic numbers to actual words, opting to just choose the word with highest probability for assignment. This does mean that some topics are repeated, however we can just collate those articles in the website.","metadata":{}},{"cell_type":"code","source":"topics = model2.get_topics()\n\n# select word with biggest confidence in set of words assigned to topic\nfor key in topics.keys():\n    max_pair = max(topics[key], key=lambda d: d[1])\n    topics[key] = max_pair\n    \nimport pickle\nwith open(\"topics.json\", \"w\") as file:\n    file.write(json.dumps(topics))\nwith open(\"topics.pkl\", \"wb\") as file:\n    pickle.dump(topics, file)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
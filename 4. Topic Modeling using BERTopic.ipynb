{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. BERTopic Modeling\n",
    "\n",
    "Below is the implementation of BERTopic used to run topic modeling the WikiNews articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install bertopic\n",
    "from bertopic import BERTopic\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Training BERT model\n",
    "\n",
    "We initially implemented topic modeling by prefitting the BERTopic model to a sample (in this case one percent) of a [pre-existing dataset of all Wikipedia articles](https://www.kaggle.com/datasets/jjinho/wikipedia-20230701), so that it can be trained specifically for wikis and therefore potentially exhibit better performance than a general BERT model. However, we were only able to train on a small sample of Wikipedia articles due to compute and memory limitations, which resulted in a model with less performance than the pretrained one (as it would represent most articles as \"unassigned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "dir = '/kaggle/input/wikipedia-20230701'\n",
    "\n",
    "file_paths = sorted(os.listdir(dir))\n",
    "file_paths.remove('wiki_2023_index.parquet')\n",
    "file_paths = [os.path.join(dir, path) for path in file_paths]\n",
    "\n",
    "fraction = 0.01\n",
    "big_data = dd.read_parquet(file_paths[0]).sample(frac=fraction, random_state=42)\n",
    "\n",
    "for file in file_paths[1:]:\n",
    "    curr = dd.read_parquet(file).sample(frac=fraction, random_state=42)\n",
    "    big_data = dd.concat([big_data, curr], ignore_index=True)\n",
    "    del curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Attempted to topic model by first premodelling topics from subset of wikipedia\n",
    "# however was not able to sample a particularly large section of wikipedia to to memory constraints\n",
    "# ended up with more topic outliers\n",
    "big_data_pd = big_data.compute()\n",
    "fit_docs = big_data_pd['text'].tolist()\n",
    "model = BERTopic()\n",
    "_ = model.fit_transform(fit_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model had around 11,000 articles marked with topic -1 (unassigned), which was worse than the pretrained model, which only labeled 6,900 articles as unassigned. Since we only had around 20,000 articles, minimizing this number is important to getting good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Initializing Pretrained BERT Model and running directly on news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = '/kaggle/input/wikinews-data-converter-2-final-stage-3/enwikinews-processed.parquet'\n",
    "\n",
    "w_data = pd.read_parquet(file_path)\n",
    "w_data.drop(columns=['page_namespace'], inplace=True)\n",
    "w_data.dropna(inplace=True)\n",
    "\n",
    "docs = w_data['page_text_extract_result'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model2 = BERTopic()\n",
    "topics, probs = model2.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics represent the assigned topic number for a given document, while probs represents documents probabilities of having that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pd.value_counts(topics) # can see number of articles assigned topic -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# add back to dataframe\n",
    "\n",
    "w_data['assigned_topic_num'] = topics\n",
    "w_data['topic_probability'] = probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Parsing Dates and generating output files\n",
    "\n",
    "Below I cleaned the date-related data in the dataframe for writing to a JSON for the website. I also outputted the topic models as a parquet for data analysis.\n",
    "The output JSON/Dataframe contains each document as a row/entry, with the documents assigned topic, topic probability, date, and article title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "topical = w_data[w_data['assigned_topic_num'] != -1]\n",
    "topical.drop(columns=['revision_id', 'page_id', 'page_text'], axis=1, inplace=True)\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_dates(text):\n",
    "    pattern = r'\\b([jJ]anuary|[fF]ebruary|[mM]arch|[aA]pril|[mM]ay|[jJ]une|[jJ]uly|[aA]ugust|[sS]eptember|[oO]ctober|[nN]ovember|[dD]ecember) (\\d{1,2}), (\\d{4})\\b'\n",
    "    match = re.findall(pattern, text[0].lower())\n",
    "    if match:\n",
    "        return {\n",
    "            \"Year\": match[0][0],\n",
    "            \"Month\": match[0][1],\n",
    "            \"Day\": match[0][2],\n",
    "            \"Hour\": 0,\n",
    "            \"Minute\": 0,\n",
    "            \"Second\": 0\n",
    "            }\n",
    "    else:\n",
    "        return np.NaN\n",
    "topical['article_date'] = topical['page_dates'].apply(clean_dates)\n",
    "topical.dropna(inplace=True)\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def parsedatetodict(timestamp):\n",
    "    try:\n",
    "        dt = parse(timestamp)\n",
    "        return {\n",
    "            \"Year\": dt.year,\n",
    "            \"Month\": dt.month,\n",
    "            \"Day\": dt.day,\n",
    "            \"Hour\": dt.hour,\n",
    "            \"Minute\": dt.minute,\n",
    "            \"Second\": dt.second\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return np.NaN\n",
    "\n",
    "\n",
    "topical['page_dates_parsed'] = topical['page_dates_parsed'].apply(lambda x: parsedatetodict(x[0]))\n",
    "topical['last_update_timestamp'] = topical['last_update_timestamp'].apply(lambda x: parsedatetodict(x[0]))\n",
    "topical.dropna(inplace=True)\n",
    "\n",
    "topical.drop(columns=['page_dates', 'page_text_extract_result'], inplace=True)\n",
    "\n",
    "topical.reset_index(drop=True, inplace=True)\n",
    "\n",
    "topical.to_json('topical_output.json', orient='records')\n",
    "topical.to_parquet('topical_output.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I produced the dict to assign the topic numbers to actual words, opting to just choose the word with highest probability for assignment. This does mean that some topics are repeated, however we can just collate those articles in the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "topics = model2.get_topics()\n",
    "\n",
    "# select word with biggest confidence in set of words assigned to topic\n",
    "for key in topics.keys():\n",
    "    max_pair = max(topics[key], key=lambda d: d[1])\n",
    "    topics[key] = max_pair\n",
    "    \n",
    "import pickle\n",
    "with open(\"topics.json\", \"w\") as file:\n",
    "    file.write(json.dumps(topics))\n",
    "with open(\"topics.pkl\", \"wb\") as file:\n",
    "    pickle.dump(topics, file)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3521629,
     "sourceId": 6146260,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 212593066,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
